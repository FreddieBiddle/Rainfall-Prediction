{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379409e4-41c2-43b3-adf8-ba025cdbf703",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feature_engineering'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfeature_engineering\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfe\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train \u001b[38;5;241m=\u001b[39m fe\u001b[38;5;241m.\u001b[39mtrain\n\u001b[1;32m     17\u001b[0m test \u001b[38;5;241m=\u001b[39m fe\u001b[38;5;241m.\u001b[39mtest\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feature_engineering'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import feature_engineering as fe\n",
    "\n",
    "train = fe.train\n",
    "test = fe.test\n",
    "\n",
    "# Preparing ensemble\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf = RandomizedSearchCV(RandomForestClassifier(), rf_param_grid, n_iter=20, cv=5, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "best_rf = rf.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "xgb = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_param_grid, n_iter=20, cv=5, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "best_xgb = xgb.best_estimator_\n",
    "\n",
    "# Adding LightGBM\n",
    "lgb = LGBMClassifier(n_estimators=200)\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "# ExtraTrees model\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Ensemble Model\n",
    "\n",
    "# ensemble = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', best_rf),\n",
    "#         ('xgb', best_xgb),\n",
    "#         ('lgb', lgb), \n",
    "#         ('et', et_model)\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression(),\n",
    "#     cv=5  # Cross-validation for meta-model\n",
    "# )\n",
    "\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('xgb', best_xgb),\n",
    "    ('lgb', lgb), \n",
    "    ('et', et_model)\n",
    "], voting='soft', weights=[2, 3, 3, 3])  # Weighted based on model performance\n",
    "    \n",
    "ensemble.fit(X_train, y_train)\n",
    "ensemble_acc = ensemble.score(X_test, y_test)\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_acc:.4f}\")\n",
    "\n",
    "# Define Neural Network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.layer1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Convert data for PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train Neural Network\n",
    "input_size = X_train.shape[1]\n",
    "model = NeuralNet(input_size)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train_nn(model, dataloader, optimizer, criterion, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            X_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step(total_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_nn(model, dataloader, optimizer, criterion)\n",
    "\n",
    "# Evaluate Neural Network\n",
    "y_pred_test = model(X_test_tensor).detach().numpy()\n",
    "y_pred_test = (y_pred_test > 0.5).astype(int)\n",
    "nn_acc = np.mean(y_pred_test == y_test_tensor.numpy())\n",
    "print(f\"Neural Network Accuracy: {nn_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
